# AlphaGo论文翻译

AlphaGo Fan使用了两种深层神经网络：一种是输出移动概率的策略网络，另一种是输出位置评估的值网络

策略网络最初由监督学习训练以准确预测人类专家的移动，然后由策略梯度强化学习进行细化，价值网络被训练来预测通过策略网络自我博弈产生的游戏赢家，一旦训练好，这些网络就交给蒙特卡罗树搜索进行仿真搜索



最后，它使用了一个简单的树搜索，它依赖于这个单一的神经网络来评估位置和样本移动，而不需要执行任何蒙特卡罗展开



利用一种新的强化学习算法，从自玩游戏中训练出alphago-zero中的神经网络。在每个位置s，执行mcts搜索，由神经网络fθ引导



更新神经网络参数θ，以最大化策略向量pt与搜索概率的相似性