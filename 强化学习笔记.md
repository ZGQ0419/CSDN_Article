# 强化学习入门笔记

## 一：绪论

&emsp;&emsp;机器学习包括：监督学习，非监督学习，**强化学习**。

### 1 强化学习与监督学习的异同：

&emsp;&emsp;**（1）**、强化学习能解决的问题：智能决策问题（序贯决策问题：连续不断的做出决策）。而监督学习解决的问题：智能感知问题。

&emsp;&emsp;**（2）、总结**：强化学习与监督学习一样都需要大量的数据进行训练，但两者所需要的数据类型不同，监督学习是需要多样化的标签数据，强化学习是需要带有回报的交互型数据。

### 2 强化学习算法的分类：

&emsp;&emsp;**(1)、根据是否依赖模型**：基于模型的强化学习算法与无模型的强化学习算法。

&emsp;&emsp;**(2)、根据策略的更新和学习方法**：基于值函数的强化学习算法、基于直接策略搜索的强化学习算法、AC方法

&emsp;&emsp;**(3)、根据环境返回的回报函数是否已知**：正向强化学习与逆向强化学习。

## 二：马尔科夫决策过程

### 1、马尔科夫性：

&emsp;&emsp;系统的下一个状态仅与当前状态有关，而与之前的状态无关。

### 2、马尔科夫过程：

&emsp;&emsp;马尔科夫过程是一个二元组（S,P），且满足：S是**有限**状态集合，P是状态转移概率。状态矩阵：P=[(P11......P1n).......(Pn1......Pnn)]。<font color=#ff00 face="宋体">它描述的是：状态 ——> 下一个状态。</font>

### 3、马尔科夫决策过程：

&emsp;&emsp;较马尔科夫过程添加了<font color=#ff00 face="宋体">动作（策略）和回报</font>；马尔科夫决策过程由元组（S,A,P,R,r）描述，其中

&emsp;&emsp;S为有限状态集；
&emsp;&emsp;A为有限动作集；
&emsp;&emsp;P为状态转移概率；
&emsp;&emsp;R为回报函数；
&emsp;&emsp;r为折扣因子，用来计算累积回报。

&emsp;&emsp;马尔科夫决策过程描述的是：<font color=#ff00 face="宋体">状态 ——> 动作 ——> 下一个状态。</font>转移概率是包含动作的（经过一个动作后也会有最终到达不同状态的情况，到达不同状态遵循概率分布）。 

&emsp;&emsp;**（1）**策略：

&emsp;&emsp;策略就是指状态到动作的映射，使用符号：圆周率 Π 表示。指的是给定在状态后，动作集上的概率分布。

&emsp;&emsp;**（2）**状态值函数：

&emsp;&emsp;当采用某策略（路线概率确定）时，累计回报在状态s处的期望值，定义为状态值函数。<font color=#ff00 face="宋体">状态值函数与策略时相对应的，因为策略决定了累计回报G的状态分布。</font>

&emsp;&emsp;**（3）**状态-行为值函数：

&emsp;&emsp;但状态和行为都确定时，求累计回报G的期望。

&emsp;&emsp;**（4）**计算状态值函数与状态-行为值函数使用的贝尔曼方程：强化学习书P23页。

&emsp;&emsp;**（5）**最优状态值函数与最优状态-行为值函数：

&emsp;&emsp;最优状态值函数是在所有策略中值最大的函数，最优状态-行为值函数是所有策略中最大的状态-行为值函数。

### 4、常用的随机策略：

&emsp;&emsp;**（1）**贪婪策略

&emsp;&emsp;其是一个确定性策略。